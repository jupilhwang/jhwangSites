<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Just Do It ! - Yolo</title>
    <link>https://jupilhwang.github.io/tags/kubernetes/</link>
    <description>Recent content in kubernetes on Just Do It ! - Yolo</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-US.UTF8</language>
    <copyright>Jupil Hwang All rights reserved</copyright>
    <lastBuildDate>Tue, 02 Feb 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://jupilhwang.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Concourse 를 통한 CI/CD 파이프라인</title>
      <link>https://jupilhwang.github.io/post/210208.concourse_pipeline-copy/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/210208.concourse_pipeline-copy/</guid>
      <description>Concourse 설치 Concourse 설치하는 방법은 여러가지가 있는데, 대표적으로 아래 두가지 방법이 있다. 테스트, 데모, PoC 용도로는 docker-compose를 사용할 수 있으며, 운영 서버로 구성하는 경우 서버에 설치하거나 Kubernetes 에 설치할 것을 권고한다.
https://concourse-ci.org/docs.html
Docker-compose docker-compose 를 사용해서 8080 포트로 concurse 를 실행할 수 있으며, docker-compose.yml 파일을 수정하여 포트나 인스턴스 수를 조정할 수 있다.
wget https://concourse-ci.org/docker-compose.yml docker-compose up -d Kubernetes / Helm 쿠버네티스에 Helm 으로 설치할 경우, https://github.com/concourse
helm repo add concourse https://concourse-charts.</description>
    </item>
    
    <item>
      <title>NSX-T Advanced Load Balancer 를 사용한 쿠버네티스 워크로드 LB</title>
      <link>https://jupilhwang.github.io/post/210208.avi_lb/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/210208.avi_lb/</guid>
      <description>Avi Networks Architecture https://avinetworks.com/docs/ako/0.8/ako-installation
Avi Controller 전체 네트워크의 로드 발랜서, WAF, 방화벽, 인증서, 로깅/모니터링 등을 담당하는 컨트럴 플레인이다.
Avi SE (Service Engine) 실제 트래픽이 전달되는 Virtual Server가 동작하는 엔진으로 데이터 플레인 역할을 한다.
Avi Kubernetes Operator (AKO) Ingress controller : 쿠버네티스 인그레스와 avi lb 의 Pool과 Virtual Server 와 동기화를 통해서 인그레스에 외부 접속 점을 만들어준다 LB Network Policy </description>
    </item>
    
    <item>
      <title>openssl로 TLS용 사설인증서 만들기</title>
      <link>https://jupilhwang.github.io/post/210205.tls_%EC%82%AC%EC%84%A4%EC%9D%B8%EC%A6%9D%EC%84%9C/</link>
      <pubDate>Mon, 01 Feb 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/210205.tls_%EC%82%AC%EC%84%A4%EC%9D%B8%EC%A6%9D%EC%84%9C/</guid>
      <description>Https 서비스를 위한 TLS 인증서 만들기 https://github.com/jupilhwang/mk-ssl-cert-key
$DOMAIN 값을 원하는 도메인(FQDN)으로 지정한다.
export DOMAIN=example.com curl https://raw.githubusercontent.com/jupilhwang/mk-ssl-cert-key/master/mk-ssl-cert-key.sh | bash - #!/bin/bash set -e SCRIPTDIR=$(cd $(dirname &amp;#34;$0&amp;#34;) &amp;amp;&amp;amp; pwd -P) : ${DOMAIN:?must be set the DNS domain root (ex: example.com)} : ${KEY_BITS:=4096} : ${DAYS:=1825} # Generate CA Certificate openssl req -new -x509 -nodes -sha256 -newkey rsa:${KEY_BITS} -days ${DAYS} -keyout ${DOMAIN}.ca.key.pkcs8 -out ${DOMAIN}.ca.crt -config &amp;lt;( cat &amp;lt;&amp;lt; EOF [ req ] prompt = no distinguished_name = dn [ dn ] C = KR O = Private CN = Autogenerated CA EOF ) # Generate Private key with CA Certificate key openssl rsa -in ${DOMAIN}.</description>
    </item>
    
    <item>
      <title>Jenkins 파이프라인 to Build Image / Push Image on Kubernetes</title>
      <link>https://jupilhwang.github.io/post/210111.jenkinsfile%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8onkubernetes/</link>
      <pubDate>Mon, 11 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/210111.jenkinsfile%ED%8C%8C%EC%9D%B4%ED%94%84%EB%9D%BC%EC%9D%B8onkubernetes/</guid>
      <description>Jenkins 설치 On K8s Prerequisites Docker pipeline plugin Google container registry for authenticate Kubernetes config Remote Docker Server docker daemon 에서 0.0.0.0:4243 을 추가해서 원격에서 tcp 로 접속할 수 있도록 한다.
# File : /etc/default/docker DOCKER_OPTS=&amp;#34;--dns 8.8.8.8 -H tcp://0.0.0.0:4243 -H unix:///var/run/docker.sock&amp;#34; # File : /lib/systemd/system/docker.service EnvironmentFile=/etc/default/docker ExecStart=/usr/bin/docker -H fd:// $DOCKER_OPTS --containerd=/run/containerd/containerd.sock 또는
# File : /etc/docker/deamon.json .. &amp;#34;hosts&amp;#34;: [&amp;#34;tcp://0.0.0.0:4243&amp;#34;, &amp;#34;unix:///var/lib/docker.sock&amp;#34;, &amp;#34;fd://&amp;#34;], .. # File : /lib/systemd/system/docker.service ExecStart=/usr/bin/docker --containerd=/run/containerd/containerd.sock 또는 ssh 로 접속하도록 할 수 있는데, 이 경우 ssh-copyid 로 미리 authorizedeky 로 등록 해 줘야한다.</description>
    </item>
    
    <item>
      <title>jumpbox on vSphere for Tanzu</title>
      <link>https://jupilhwang.github.io/post/201223-vsphere-jumpbox-for-tanzu/</link>
      <pubDate>Wed, 23 Dec 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/201223-vsphere-jumpbox-for-tanzu/</guid>
      <description>이 문서는 vsphere 환경에서 tanzu를 설치/설정하기 위해 사용하는 jumpbox 를 손쉽게 사용하기 위한 문서이다.
deploy OVA 대체로 Private Cloud이든 Public Cloud이든 동일한 작업환경을 위해서 Jumpbox를 설치해서작업을 하며 Jumpbox로는 ubuntu server 를 많이 사용한다. 여기서도 ubuntu-server-20.04.1 (LTS) 을 기준으로 설명한다.
https://cloud-images.ubuntu.com/focal/current/focal-server-cloudimg-amd64.ova
iso로 실제 ubuntu-server를 설치해도 되고, 이미 export 해놓은 ova를 업로드하여 사용할 수 있지만, Public internet이 되는 환경이라면 이미 만들어진 ubuntu-cloudimage 를 사용하는 것도 좋다.
ubuntu cloud image는 https://cloud-images.ubuntu.com 에서 받을 수 있다.</description>
    </item>
    
    <item>
      <title>TKG 1.2 설치/설정 : quick start</title>
      <link>https://jupilhwang.github.io/post/201101-tkg-quick-start/</link>
      <pubDate>Sun, 01 Nov 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/201101-tkg-quick-start/</guid>
      <description>TKG Components Storage Class vSphere 에서 Storage를 사용하기 위해 Tag 기반의 policy 를 적용한 Datastore 를 사용한다. govc tags.category.create tkg-storage-category govc tags.create -c tkg-storage-category tkg-storage govc tags.attach tkg-storage /Datacenter/datastore/LUN01 tkg cluster 생성 시 자동으로 만들어 진 default sc 를 삭제하고 새로 생성한다. k delete sc default k apply -f -&amp;lt;&amp;lt;-EOF kind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: default annotations: storageclass.kubernetes.io/is-default-class: &amp;#34;true&amp;#34; provisioner: csi.vsphere.vmware.com parameters: storagepolicyname: &amp;#34;TKG Storage Policy&amp;#34; # optional fstype: ext4 EOF MetalLB k apply -f https://raw.</description>
    </item>
    
    <item>
      <title>TKG에 Antrea CNI 사용하기</title>
      <link>https://jupilhwang.github.io/post/200625_antrea_on_tkg/</link>
      <pubDate>Thu, 25 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/200625_antrea_on_tkg/</guid>
      <description>Antrea Antrea 는 OVS(Open vSwitch)를 기본으로 하는 Kubernetes용 네트워킹 솔루션 이다. Layer3/4 역할을 담당하며, 보안과 운영상의 도움을 주도록 설계되었다.
https://github.com/vmware-tanzu/antrea/
Prerequisites NodeIPAMController : K8s에서 enabled 되어 있어야 한다. kubeadm 으로 클러스터를 만들 떄 &amp;ndash;pod-network-cidr 옵션이 있어야 한다 Open vSwitch kernel module 이 노드에 있어야 한다 Willam Lam 의 Antrea 설치 문서와 내부 자료를 참고했다.
설치 tkg의 새로운 Plan을 만든다. tkg는 ~/.tkg/providers/infrastructure-vsphere/v0.6.4/cluster-template-*.yaml 을 plan으로 사용하기 때문에 새로운 yaml을 만든다.
아래에서는 dev 파일을 기본으로 작업하는데, dev와 prod를 동시에 작업해 주는게 좋다.</description>
    </item>
    
    <item>
      <title>TKG에 MetalLB</title>
      <link>https://jupilhwang.github.io/post/200625_metallb/</link>
      <pubDate>Sat, 20 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/200625_metallb/</guid>
      <description>MetalLB # context kubectl config use-context my-cluster-admin@my-cluster ## MetalLB는 kube-proxy의 IPVS를 사용할 때 Strict ARP가 필요하다 kubectl get configmap kbue-proxy -n kube-system -o yaml | sed -e &amp;#34;s/strictARP: false/strictARP: true/&amp;#34; | kubectl apply -f - -n kube-system kubectl create ns metallb-system # Deploy MetalLB kubectl apply -f https://raw.githubusercontent.com/google/metallb/v0.9.3/manifests/metallb.yaml -n metallb-system podsecuritypolicy.policy/controller created podsecuritypolicy.policy/speaker created serviceaccount/controller created serviceaccount/speaker created clusterrole.rbac.authorization.k8s.io/metallb-system:controller created clusterrole.rbac.authorization.k8s.io/metallb-system:speaker created role.rbac.authorization.k8s.io/config-watcher created role.rbac.authorization.k8s.io/pod-lister created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:controller created clusterrolebinding.rbac.authorization.k8s.io/metallb-system:speaker created rolebinding.</description>
    </item>
    
    <item>
      <title>CF for K8s</title>
      <link>https://jupilhwang.github.io/post/200515_cf-for-k8s/</link>
      <pubDate>Fri, 15 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/200515_cf-for-k8s/</guid>
      <description>Cloudfoundry 는 컨테이너 기반의 애플리케이션을 위한 플랫폼으로 PaaS 시장을 선도하고 있다. Heroku와 더불어 PaaS의 양대 산맥이라 할 수 있을 것이다. Cloudfoundry 는 자체 개발한 Garden이라고 하는 컨테이너 오케스트레이션 툴을 사용하였다. Kubernetes 가 등장하고 나서, 이 컨테이너 오케스트레이션 툴을 Kuberentes 으로 대체하는 프로젝트가 등장하였고, 마침내 cf-for-k8s 가 나왔다. 기존에 애플리케이션을 개발하여 손쉽게 배포하여 테스트하고 운영 하던 경험을 그대로 쿠버네티스에서도 경험할 수 있다.
CloudFoundry for Kubernetes https://tanzu.vmware.com/developer/guides/kubernetes/cf4k8s-gs/
그 동안 BOSH / Deigo 기반의 Cloudfoundry (이하 CF) 를 Kubernetes 기반으로 사용할 수 있다.</description>
    </item>
    
    <item>
      <title>vSphere with Kubernetes - Tanzu Kubernetes Cluster : 선언적(Declarative) GitOPS CD</title>
      <link>https://jupilhwang.github.io/post/vsphere-with-kubernetes-gitops/</link>
      <pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/vsphere-with-kubernetes-gitops/</guid>
      <description>GitOps ArgoCD Installation kubectl create ns argocd kubectl -n argocd apply -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml Disable auth flag kubectl patch deploy argocd-server -n argocd -p &amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;add&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/template/spec/containers/0/command/-&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;--disable-auth&amp;#34;}]&amp;#39; --type json CD for Tanzu Kubernetes Cluster </description>
    </item>
    
    <item>
      <title>vSphere with Kubernetes의 Master/Worker Node SSH 접속하기</title>
      <link>https://jupilhwang.github.io/post/vsphere-with-kuberentes%EC%9D%98-master_worker-node-%EC%A0%91%EC%86%8D%ED%95%98%EA%B8%B0/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/vsphere-with-kuberentes%EC%9D%98-master_worker-node-%EC%A0%91%EC%86%8D%ED%95%98%EA%B8%B0/</guid>
      <description>vSphere with Kubernetes vSphere 7.0 vSphere 7.0의 가장 큰 차이라고 한다면 당연하게 Kubernetes의 지원이다. 기존에 가상 머신, 스토리지, 네트워크의 SDDC부분에 중점을 두었다면, 쿠버네티스를 사용한 메니지먼트로 다양한 환경에서 동일한 메니페스트를 통해 관리를 할 수 있도록 한 것이다. Supervisor Cluster vSphere 7.0 의 Supervisor 클러스터를 통해서 vSphere 전반적인 가상머신, vSphere Pod, Tanzu Kubernetes Cluster를 생성/삭제 등의 라이프사이클을 관리한다. 사용자/그룹 관리, SSO연계, 퍼미션 설정, 네임스페이스 관리 등을 한다.
Supervisor 노드 접속 Supervisor노드에 접속하기 위해서</description>
    </item>
    
    <item>
      <title>Kind를 이용한 쿠버네티스 클러스터 만들기</title>
      <link>https://jupilhwang.github.io/post/kind%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EB%A7%8C%EB%93%A4%EA%B8%B0/</link>
      <pubDate>Sun, 26 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/kind%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-%EC%BF%A0%EB%B2%84%EB%84%A4%ED%8B%B0%EC%8A%A4-%ED%81%B4%EB%9F%AC%EC%8A%A4%ED%84%B0-%EB%A7%8C%EB%93%A4%EA%B8%B0/</guid>
      <description>Kubernetes 개발환경 Vagrant vs Minikube vs Kind Local 환경에서 K8s를 사용하기 위해서 다양한 방법을 사용 할 수 있다.
Vagrant Vagrant는 HashCorp에서 만든 프로비저닝 툴로, VagrantFile에 기본 이미지와 생성된 VM에 필요한 설정을 미리 Code화 해서 제공하는 툴이다.
# vagrant cli 가 설치 되어야 한다. vagrant version # git cli 가 설치 되면 좋다. git version # 기본 가상화 솔루션으로 VirtualBox를 사용하기 때문에, VirtualBox가 미리 설치가 되어 있어야 한다. Hyper-V나 다른 가상화 툴을 사용할 수 있다.</description>
    </item>
    
    <item>
      <title>Istio 1.5로 구성하는 서비스메시</title>
      <link>https://jupilhwang.github.io/post/istio1.5%EB%A1%9C_%EA%B5%AC%EC%84%B1%ED%95%98%EB%8A%94_%EC%84%9C%EB%B9%84%EC%8A%A4%EB%A9%94%EC%8B%9C/</link>
      <pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>https://jupilhwang.github.io/post/istio1.5%EB%A1%9C_%EA%B5%AC%EC%84%B1%ED%95%98%EB%8A%94_%EC%84%9C%EB%B9%84%EC%8A%A4%EB%A9%94%EC%8B%9C/</guid>
      <description>Istio로 구성하는 서비스 메시 이전 Architecture 일반적으로 Istio의 구성요소라고 하면, 아래와 같이 이야기한다.
Envoy Proxy Mixer Pilot Galley Citadel 하지만, Mixer로 인한 네트워크 레이턴시 성능 이슈가 있어서, 아키텍처가 변경이 되었는데 Mixer가 없어지고 Telemetry로 변경이 되고, Envoy Proxy 에서 바로 모니터링 솔루션우로 메트릭을 전송하는 형태로 변경이 되었다.
달라진 Architecture </description>
    </item>
    
    <item>
      <title>Kubernetes 에서 Kubesphere를 이용한 DevOps</title>
      <link>https://jupilhwang.github.io/post/kubernetes%EC%97%90%EC%84%9C-kubesphere%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-devops/</link>
      <pubDate>Wed, 25 Mar 2020 10:46:06 +0900</pubDate>
      
      <guid>https://jupilhwang.github.io/post/kubernetes%EC%97%90%EC%84%9C-kubesphere%EB%A5%BC-%EC%9D%B4%EC%9A%A9%ED%95%9C-devops/</guid>
      <description>Developer Portal with Kubesphere kubesphere (https://github.com/kubesphere/kubesphere) 의 오픈소스 프로젝트로 쿠버네티스 클러스터에서 사용할 수 있는 웹 UI를 제공합니다. 워크스페이스/프로젝트 단위로 워크로드를 관리하고, Jenkins를 통한 파이프라인도 제공한다. 내부적으로 프로메테우스와 ElasticSearch 를 통한 모니터링/로깅을 제공한다. OpenPitrix 를 사용한 App Store도 제공한다.
공식 사이트: https://kubesphere.io
Environments PKS(Pivotal Container Service) 1.6.1 NSX-T 2.4.3 vSphere 6.7U3 PKS Installation with EPMC(Enterprise PKS Management Console) Deploye PKS Management Console Configuration Deploy Create K8s Cluster 클러스터 생성
pks create-cluster k8s --external-hostname demo-cluster.</description>
    </item>
    
  </channel>
</rss>
